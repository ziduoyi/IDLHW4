import numpy as np


class CTC(object):

    def __init__(self, BLANK=0):
        """
        Initialize instance variables

        Argument(s)
        -----------
        BLANK (int, optional): blank label index. Default 0.
        """

        # No need to modify
        self.BLANK = BLANK

    def extend_target_with_blank(self, target):
        """Extend target sequence with blank.

        Input
        -----
        target: (np.array, dim = (target_len,))
                target output
        ex: [B,IY,IY,F]

        Return
        ------
        extSymbols: (np.array, dim = (2 * target_len + 1,))
                    extended target sequence with blanks
        ex: [-,B,-,IY,-,IY,-,F,-]

        skipConnect: (np.array, dim = (2 * target_len + 1,))
                    skip connections
        ex: [0,0,0,1,0,0,0,1,0]
        """

        extended_symbols = [self.BLANK]
        for symbol in target:
            extended_symbols.append(symbol)
            extended_symbols.append(self.BLANK)

        N = len(extended_symbols)
        
        # -------------------------------------------->
        # TODO
        # <---------------------------------------------

        # -------------------------------------------->
        # TODO: Develop the logic for `skip_connect`.
        # This array should flag positions where a direct transition, bypassing an adjacent
        # blank or a repeated label, is permissible according to CTC rules.
        # Consider the conditions under which a 'skip' is allowed in the extended symbol sequence.
        # <---------------------------------------------
        skip_connect = np.zeros(N, dtype=np.int32)
        for s in range(N):
            if s >= 2 and extended_symbols[s] != self.BLANK and extended_symbols[s] != extended_symbols[s - 2]:
                skip_connect[s] = 1

        return extended_symbols, skip_connect

    def get_forward_probs(self, logits, extended_symbols, skip_connect):
        """Compute forward probabilities.

        Input
        -----
        logits: (np.array, dim = (input_len, len(Symbols)))
                predict (log) probabilities

                To get a certain symbol i's logit as a certain time stamp t:
                p(t,s(i)) = logits[t, qextSymbols[i]]

        extSymbols: (np.array, dim = (2 * target_len + 1,))
                    extended label sequence with blanks

        skipConnect: (np.array, dim = (2 * target_len + 1,))
                    skip connections

        Return
        ------
        alpha: (np.array, dim = (input_len, 2 * target_len + 1))
                forward probabilities

        """

        S, T = len(extended_symbols), len(logits)
        alpha = np.zeros(shape=(T, S))

        # -------------------------------------------->
        # TODO: Initialize the starting probabilities for the first time step.
		# TODO: Intialize alpha[0][0]
		# TODO: Intialize alpha[0][1]
        # This involves setting the initial values for the first two extended symbols.
        #
		# TODO: Compute all values for alpha[t][sym] where 1 <= t < T and 1 <= sym < S (assuming zero-indexing)
        # TODO: Implement the iterative computation for `alpha` values across all subsequent time steps.
        # IMP: Remember to check for skipConnect when calculating alpha
        # For each `alpha[t][sym]`, consider the possible paths from the previous time step `t-1`.
        # These paths typically include transitions from the same symbol at `t-1`, and the preceding symbol at `t-1`.
        # Critically, incorporate the `skipConnect` information to allow for additional transitions
        # from a symbol two positions prior in the extended sequence, under specific CTC rules.
        # Ensure proper indexing and multiplication with the relevant logit for the current state.
        # <---------------------------------------------

        alpha[0, 0] = logits[0, extended_symbols[0]]
        if S > 1:
            alpha[0, 1] = logits[0, extended_symbols[1]]

        for t in range(1, T):
            for s in range(S):
                total = alpha[t - 1, s]
                if s - 1 >= 0:
                    total += alpha[t - 1, s - 1]
                if s - 2 >= 0 and skip_connect[s]:
                    total += alpha[t - 1, s - 2]
                alpha[t, s] = logits[t, extended_symbols[s]] * total
        return alpha

    def get_backward_probs(self, logits, extended_symbols, skip_connect):
        """Compute backward probabilities.

        Input
        -----
        logits: (np.array, dim = (input_len, len(symbols)))
                predict (log) probabilities

                To get a certain symbol i's logit as a certain time stamp t:
                p(t,s(i)) = logits[t,extSymbols[i]]

        extSymbols: (np.array, dim = (2 * target_len + 1,))
                    extended label sequence with blanks

        skipConnect: (np.array, dim = (2 * target_len + 1,))
                    skip connections

        Return
        ------
        beta: (np.array, dim = (input_len, 2 * target_len + 1))
                backward probabilities

        """
        S, T = len(extended_symbols), len(logits)
        beta = np.zeros(shape=(T, S))

        # -------------------------------------------->
        # TODO: Establish the terminating probabilities at the last time step.
        # This typically involves setting initial `beta` values for the last two extended symbols.
        #
        # TODO: Proceed with backward iterative computation for `beta` values through time.
        # For each `beta[t][sym]`, determine the contributions from future states at time `t+1`.
        # These contributions usually involve the same symbol at `t+1` and the next symbol at `t+1`.
        # Integrate the `skipConnect` logic to account for transitions from states
        # two positions ahead in the extended sequence, where allowed by CTC rules.
        # Careful consideration of indexing is necessary to prevent out-of-bounds access.
        # Each computed beta value should be adjusted by the current symbol's logit at the current time step.
        # <--------------------------------------------

        # -------------------------------------------->
        # TODO
        # <--------------------------------------------


        beta[T - 1, S - 1] = 1.0
        if S - 2 >= 0:
            beta[T - 1, S - 2] = 1.0

        for t in range(T - 2, -1, -1):
            for s in range(S):
                total = beta[t + 1, s] * logits[t + 1, extended_symbols[s]]
                if s + 1 < S:
                    total += beta[t + 1, s + 1] * logits[t + 1, extended_symbols[s + 1]]
                if s + 2 < S and skip_connect[s + 2]:
                    total += beta[t + 1, s + 2] * logits[t + 1, extended_symbols[s + 2]]
                beta[t, s] = total
        return beta

    def get_posterior_probs(self, alpha, beta):
        """Compute posterior probabilities.

        Input
        -----
        alpha: (np.array, dim = (input_len, 2 * target_len + 1))
                forward probability

        beta: (np.array, dim = (input_len, 2 * target_len + 1))
                backward probability

        Return
        ------
        gamma: (np.array, dim = (input_len, 2 * target_len + 1))
                posterior probability

        """

        [T, S] = alpha.shape
        gamma = np.zeros(shape=(T, S))

        # -------------------------------------------->
        # TODO: Calculate the unnormalized joint probability for each (time, symbol) pair.
        # This involves combining the `alpha` and `beta` probabilities at each point.
        #
        # TODO: Normalize these joint probabilities at each time step.
        # For each time step, sum all unnormalized joint probabilities across all extended symbols.
        # Then, divide each individual unnormalized joint probability by this sum to ensure
        # that the posteriors for a given time step sum to one.
        # Remember to add a small numerical stability constant (epsilon) to the denominator.
        # <---------------------------------------------
        eps = 1e-12
        for t in range(T):
            unnorm = alpha[t] * beta[t]
            Z = np.sum(unnorm) + eps
            gamma[t] = unnorm / Z
        return gamma


class CTCLoss(object):

    def __init__(self, BLANK=0):
        """
        Initialize instance variables

        Argument(s)
        -----------
        BLANK (int, optional): blank label index. Default 0.

        """
        # -------------------------------------------->
        # No need to modify
        super(CTCLoss, self).__init__()

        self.BLANK = BLANK
        self.gammas = []
        self.ctc = CTC()
        # <---------------------------------------------

    def __call__(self, logits, target, input_lengths, target_lengths):

        # No need to modify
        return self.forward(logits, target, input_lengths, target_lengths)

    def forward(self, logits, target, input_lengths, target_lengths):
        """CTC loss forward

        Computes the CTC Loss by calculating forward, backward, and
        posterior proabilites, and then calculating the avg. loss between
        targets and predicted log probabilities

        Input
        -----
        logits [np.array, dim=(seq_length, batch_size, len(symbols)]:
            log probabilities (output sequence) from the RNN/GRU

        target [np.array, dim=(batch_size, padded_target_len)]:
            target sequences

        input_lengths [np.array, dim=(batch_size,)]:
            lengths of the inputs

        target_lengths [np.array, dim=(batch_size,)]:
            lengths of the target

        Returns
        -------
        loss [float]:
            avg. divergence between the posterior probability and the target

        """

        # No need to modify
        self.logits = logits
        self.target = target
        self.input_lengths = input_lengths
        self.target_lengths = target_lengths

        #####  IMP:
        #####  Output losses should be the mean loss over the batch

        # No need to modify
        B, _ = target.shape
        total_loss = []
        self.extended_symbols = []

        for batch_itr in range(B):
            # -------------------------------------------->
            # Computing CTC Loss for single batch
            # Process:
            #     Truncate the target to target length
            #     Truncate the logits to input length
            #     Extend target sequence with blank
            #     Compute forward probabilities
            #     Compute backward probabilities
            #     Compute posteriors using total probability function
            #     Compute expected divergence for each batch and store it in totalLoss
            #     Take an average over all batches and return final result
            # <---------------------------------------------

            # -------------------------------------------->
            # TODO
            # <---------------------------------------------
            tgt = target[batch_itr][:target_lengths[batch_itr]]
            logit_b = logits[:input_lengths[batch_itr], batch_itr, :]

            ext, skip = self.ctc.extend_target_with_blank(tgt)

            alpha = self.ctc.get_forward_probs(logit_b, ext, skip)
            beta = self.ctc.get_backward_probs(logit_b, ext, skip)
            gamma = self.ctc.get_posterior_probs(alpha, beta)
            self.gammas.append(gamma)

            loss_b = 0.0
            Tb, S = gamma.shape
            for t in range(Tb):
                for s in range(S):
                    c = ext[s]
                    p = logit_b[t, c]
                    loss_b += - gamma[t, s] * np.log(p + 1e-12)

            total_loss.append(loss_b)

        total_loss = np.sum(total_loss) / B

        return total_loss

    def backward(self):
        """
        CTC loss backard

        Calculate the gradients w.r.t the parameters and return the derivative 
        w.r.t the inputs, xt and ht, to the cell.

        Input
        -----
        logits [np.array, dim=(seqlength, batch_size, len(Symbols)]:
            log probabilities (output sequence) from the RNN/GRU

        target [np.array, dim=(batch_size, padded_target_len)]:
            target sequences

        input_lengths [np.array, dim=(batch_size,)]:
            lengths of the inputs

        target_lengths [np.array, dim=(batch_size,)]:
            lengths of the target

        Returns
        -------
        dY [np.array, dim=(seq_length, batch_size, len(extended_symbols))]:
            derivative of divergence w.r.t the input symbols at each time

        """

        # No need to modify
        T, B, C = self.logits.shape
        dY = np.full_like(self.logits, 0)

        for batch_itr in range(B):
            # -------------------------------------------->
            # Computing CTC Derivative for single batch
            # Process:
            #     Truncate the target to target length
            target = self.target[batch_itr][:self.target_lengths[batch_itr]]
            #     Truncate the logits to input length
            logit = self.logits[:self.input_lengths[batch_itr], batch_itr]
            #     Extend target sequence with blank
            extended, _ = self.ctc.extend_target_with_blank(target)
            #     Compute derivative of divergence and store them in dY
            
            # <---------------------------------------------
            gamma = self.gammas[batch_itr]
            S = gamma.shape[1]

            for t in range(self.input_lengths[batch_itr]):
                for s in range(S):
                    c = extended[s]
                    p = logit[t, c]
                    dY[t, batch_itr, c] += - gamma[t, s] / (p + 1e-12)

        return dY
